{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Matrix Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import shutil\n",
    "import faiss\n",
    "\n",
    "\n",
    "#FILENAMES\n",
    "lexi = \"/content/drive/MyDrive/DSRsquared/3_Million/Lexicon_3_Mill.csv\"\n",
    "########################################################\n",
    "print(\"Loading model\")\n",
    "model = SentenceTransformer('multi-qa-mpnet-base-cos-v1')\n",
    "Lexicon = pandas.read_csv(lexi, usecols=[0], header = None)\n",
    "\n",
    "#extract words\n",
    "words= Lexicon.iloc[:, 0]\n",
    "print(\"Encoding\")\n",
    "vectors = model.encode(words, convert_to_numpy=True).astype(\"float32\")\n",
    "PCA = faiss.PCAMatrix(vectors.shape[1], 100)\n",
    "PCA.train(vectors)\n",
    "low_vectors = PCA.apply_py(vectors)\n",
    "\n",
    "\n",
    "numpy.save(\"Embeddings_Full.npy\", low_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Clustering and Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "import numpy as np\n",
    "import faiss\n",
    "import pandas as pd\n",
    "import shutil\n",
    "\n",
    "\n",
    "########################################################\n",
    "\n",
    "# Load vectors\n",
    "print(\"Loading vectors\\n\")\n",
    "vectors = np.load(\"Embeddings_Full.npy\")\n",
    "\n",
    "# Normalize vectors\n",
    "print(\"Normalizing\\n\")\n",
    "vectors = normalize(vectors)\n",
    "\n",
    "n_clusters = int(vectors.shape[0]/10)\n",
    "\n",
    "# Generate cluster labels\n",
    "print(\"Using KMeans\")\n",
    "res = faiss.StandardGpuResources()\n",
    "kmeans = faiss.Kmeans(vectors.shape[1], n_clusters,\n",
    "                      niter = 150,nredo =10, gpu = True, verbose = True)\n",
    "kmeans.train(vectors)\n",
    "_, cluster_assignments = kmeans.index.search(vectors, 1)\n",
    "clusters = cluster_assignments.flatten()\n",
    "centroids = kmeans.centroids\n",
    "\n",
    "# Save cluster label\n",
    "clusters = np.load(\"Clusters_Full.npy\")\n",
    "centroids = np.load(\"Centroids_Full.npy\")\n",
    "\n",
    "np.save(\"Clusters_Full.npy\", clusters)\n",
    "np.save(\"Centroids_Full.npy\", centroids)\n",
    "\n",
    "\n",
    "# Append cluster labels to lexicon and save\n",
    "print(\"\\nWriting to file\")\n",
    "Lexicon = pandas.read_csv('/content/drive/MyDrive/DSRsquared/4_Million/Lexicon_4_Mill.csv', header = None)\n",
    "Lexicon['Clusters'] = clusters\n",
    "Lexicon.to_csv(\"Lexicon_Full_Barrels.csv\", header=None, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from cuml.manifold import TSNE as cumlTSNE\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import normalize\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "vectors = np.load(\"/content/drive/MyDrive/DSRsquared/1_Million/Embeddings_1.npy\")\n",
    "vectors = normalize(vectors)\n",
    "\n",
    "# Reduce dimensions with PCA\n",
    "print(\"Reducing dimensions with PCA\\n\")\n",
    "pca = faiss.PCAMatrix(vectors.shape[1], 20)\n",
    "pca.train(vectors)\n",
    "low_vectors = pca.apply_py(vectors)\n",
    "\n",
    "\n",
    "print(\"Reducing dimensionality to 2D for visualization\\n\")\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=200,\n",
    "                learning_rate = 1000,\n",
    "                metric = 'cosine', init = 'pca',\n",
    "                early_exaggeration = 30,\n",
    "                n_iter = 1000)\n",
    "vectors_2d = tsne.fit(low_vectors)\n",
    "np.save(\"vectors_2D_tSNE.npy\", vectors_2d)\n",
    "\n",
    "# Reduce dimensionality for 3D visualization\n",
    "print(\"Reducing dimensionality to 3D for visualization\\n\")\n",
    "tsne = TSNE(n_components=3, random_state=42, perplexity=50,\n",
    "                learning_rate = 400, max_iter = 700,\n",
    "                metric = 'cosine', init = 'pca', method = 'barnes_hut',\n",
    "                n_jobs=-1, n_iter_without_progress=30,\n",
    "                verbose=2)\n",
    "vectors_3d = tsne.fit_transform(low_vectors)\n",
    "np.save(\"vectors_3D_tSNE.npy\", vectors_3d)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3D Point Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import plotly.express as px\n",
    "import numpy\n",
    "\n",
    "Lexicon = pandas.read_csv(\"/content/drive/MyDrive/DSRsquared/1_Million/Lexicon_1_Barrels.csv\",\n",
    "                          usecols=[0])\n",
    "words = Lexicon.iloc[:, 0]\n",
    "\n",
    "\n",
    "Cluster_Frame = pandas.read_csv(\"/content/drive/MyDrive/DSRsquared/1_Million/Clusters_1_Barrels.csv\",\n",
    "                          usecols=[3])\n",
    "clusters = Cluster_Frame.iloc[:, 0]\n",
    "vectors_3d = numpy.load(\"vectors_3D_tSNE.npy\")\n",
    "\n",
    "# Create a DataFrame for visualization with hover text\n",
    "df = pd.DataFrame({\n",
    "    'x': vectors_3d[:, 0],\n",
    "    'y': vectors_3d[:, 1],\n",
    "    'z': vectors_3d[:, 2],\n",
    "    'Cluster': clusters,\n",
    "    'Word': words  # Add words for hover text\n",
    "})\n",
    "\n",
    "# Plot interactive 3D scatter\n",
    "fig = px.scatter_3d(\n",
    "    df,\n",
    "    x='x', y='y', z='z',\n",
    "    color='Cluster',\n",
    "    color_continuous_scale=px.colors.qualitative.Set3,\n",
    "    title=\"3D Visualization of Clusters\",\n",
    "    opacity=0.7,\n",
    "    hover_name='Word'  # Use 'Word' column for hover text\n",
    ")\n",
    "fig.update_traces(marker=dict(size=3))\n",
    "\n",
    "fig.show()\n",
    "fig.write_html(\"3D_Cloud.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import numpy\n",
    "\n",
    "Lexicon = pd.read_csv(\"/content/drive/MyDrive/DSRsquared/1_Million/Lexicon_1_Barrels.csv\",\n",
    "                          usecols=[0], header = None)\n",
    "words = Lexicon.iloc[:, 0]\n",
    "\n",
    "Cluster_Frame = pd.read_csv(\"/content/drive/MyDrive/DSRsquared/1_Million/Lexicon_1_Barrels.csv\",\n",
    "                          usecols=[3], header = None)\n",
    "clusters = Cluster_Frame.iloc[:, 0]\n",
    "vectors_2d = numpy.load(\"vectors_2D_tSNE.npy\")\n",
    "\n",
    "# Create a DataFrame for 2D visualization with hover text\n",
    "df_2d = pd.DataFrame({\n",
    "    'x': vectors_2d[:, 0],  # Use the first dimension\n",
    "    'y': vectors_2d[:, 1],  # Use the second dimension\n",
    "    'Cluster': clusters,\n",
    "    'Word': words  # Add words for hover text\n",
    "})\n",
    "\n",
    "# Plot interactive 2D scatter\n",
    "fig_2d = px.scatter(\n",
    "    df_2d,\n",
    "    x='x', y='y',\n",
    "    color='Cluster',\n",
    "    color_continuous_scale=px.colors.qualitative.Set3,\n",
    "    title=\"2D Visualization of Clusters\",\n",
    "    hover_name='Word',  # Use 'Word' column for hover text\n",
    "    opacity=0.7\n",
    ")\n",
    "fig_2d.update_traces(marker=dict(size=5))\n",
    "\n",
    "fig_2d.show()\n",
    "fig_2d.write_html(\"2D_Map.html\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Barreling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import threading\n",
    "from collections import defaultdict\n",
    "import ijson\n",
    "import logging\n",
    "from typing import Dict, Any, Iterator\n",
    "import queue\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class BarrelIndexer:\n",
    "    def __init__(self, cluster_file: str, inverted_index_file: str, output_dir: str, chunk_size: int = 10000):\n",
    "        self.cluster_file = cluster_file\n",
    "        self.inverted_index_file = inverted_index_file\n",
    "        self.output_dir = output_dir\n",
    "        self.chunk_size = chunk_size\n",
    "        self.barrel_locks = defaultdict(threading.Lock)\n",
    "        self.barrel_buffers = defaultdict(dict)\n",
    "        self.buffer_size = 1000  # Number of entries to buffer before writing\n",
    "        \n",
    "    def load_clusters(self) -> Dict[int, int]:\n",
    "        \"\"\"Load cluster assignments efficiently\"\"\"\n",
    "        df = pd.read_csv(self.cluster_file, usecols=[1, 2])\n",
    "        return dict(zip(df['ID'], df['Cluster']))\n",
    "\n",
    "    def stream_inverted_index(self) -> Iterator[tuple]:\n",
    "        \"\"\"Stream the inverted index file in chunks\"\"\"\n",
    "        with open(self.inverted_index_file, 'rb') as file:\n",
    "            parser = ijson.parse(file)\n",
    "            current_key = None\n",
    "            current_value = None\n",
    "            \n",
    "            for prefix, event, value in parser:\n",
    "                if prefix.endswith('.key'):\n",
    "                    current_key = value\n",
    "                elif prefix.endswith('.value'):\n",
    "                    current_value = value\n",
    "                    if current_key is not None:\n",
    "                        yield (int(current_key), current_value)\n",
    "                        current_key = None\n",
    "                        current_value = None\n",
    "\n",
    "    def write_barrel_chunk(self, barrel: int, data: Dict[str, Any]):\n",
    "        \"\"\"Write a chunk of data to a barrel file with locking\"\"\"\n",
    "        barrel_file = os.path.join(self.output_dir, f\"{barrel}.json\")\n",
    "        \n",
    "        with self.barrel_locks[barrel]:\n",
    "            try:\n",
    "                existing_data = {}\n",
    "                if os.path.exists(barrel_file) and os.path.getsize(barrel_file) > 0:\n",
    "                    with open(barrel_file, 'r') as f:\n",
    "                        existing_data = json.load(f)\n",
    "                \n",
    "                existing_data.update(data)\n",
    "                \n",
    "                with open(barrel_file, 'w') as f:\n",
    "                    json.dump(existing_data, f, separators=(',', ':'))\n",
    "                    \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error writing to barrel {barrel}: {str(e)}\")\n",
    "                raise\n",
    "\n",
    "    def process_chunk(self, chunk: Dict[str, Any], barrels: Dict[int, int]):\n",
    "        \"\"\"Process a chunk of the inverted index\"\"\"\n",
    "        barrel_chunks = defaultdict(dict)\n",
    "        \n",
    "        for index, value in chunk.items():\n",
    "            index_int = int(index)\n",
    "            barrel = barrels.get(index_int)\n",
    "            \n",
    "            if barrel is None:\n",
    "                logger.warning(f\"No barrel found for index {index_int}\")\n",
    "                continue\n",
    "                \n",
    "            barrel_chunks[barrel][index] = value\n",
    "            \n",
    "        # Write accumulated chunks for each barrel\n",
    "        for barrel, data in barrel_chunks.items():\n",
    "            self.write_barrel_chunk(barrel, data)\n",
    "\n",
    "    def process(self, max_workers: int = 4):\n",
    "        \"\"\"Main processing method with parallel execution\"\"\"\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "        \n",
    "        # Load cluster assignments\n",
    "        logger.info(\"Loading cluster assignments...\")\n",
    "        barrels = self.load_clusters()\n",
    "        \n",
    "        # Create thread pool\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            current_chunk = {}\n",
    "            futures = []\n",
    "            \n",
    "            # Process the inverted index in streaming fashion\n",
    "            for index, value in tqdm(self.stream_inverted_index(), desc=\"Processing entries\"):\n",
    "                current_chunk[str(index)] = value\n",
    "                \n",
    "                if len(current_chunk) >= self.chunk_size:\n",
    "                    chunk_to_process = current_chunk\n",
    "                    current_chunk = {}\n",
    "                    futures.append(\n",
    "                        executor.submit(self.process_chunk, chunk_to_process, barrels)\n",
    "                    )\n",
    "                    \n",
    "            # Process any remaining entries\n",
    "            if current_chunk:\n",
    "                futures.append(\n",
    "                    executor.submit(self.process_chunk, current_chunk, barrels)\n",
    "                )\n",
    "            \n",
    "            # Wait for all tasks to complete\n",
    "            for future in tqdm(futures, desc=\"Waiting for tasks\"):\n",
    "                future.result()\n",
    "\n",
    "def main():\n",
    "    indexer = BarrelIndexer(\n",
    "        cluster_file=\".\\\\Barrels\\\\Clusters_KMEANS.csv\",\n",
    "        inverted_index_file=\"Inverted_index.json\",\n",
    "        output_dir=\".\\\\Barrels\\\\Index_Barrels\",\n",
    "        chunk_size=10000\n",
    "    )\n",
    "    indexer.process(max_workers=4)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonEnvironment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
