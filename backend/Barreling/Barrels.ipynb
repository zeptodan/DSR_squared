{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Matrix Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import spacy\n",
    "import numpy\n",
    "from tqdm import tqdm\n",
    "\n",
    "#FILENAMES\n",
    "lexi = \"LexiconFinal.csv\"\n",
    "spacy_model = \"en_core_web_lg\"\n",
    "########################################################\n",
    "nlp = spacy.load(spacy_model)\n",
    "Lexicon = pandas.read_csv(lexi, usecols=[1, 2])\n",
    "\n",
    "#extract words\n",
    "words= Lexicon.iloc[:, 0]\n",
    "\n",
    "\n",
    "#Generate a list of vectors\n",
    "vectors = []\n",
    "for word in tqdm(words, desc = \"Loading vectors\"):\n",
    "  vectors.append(nlp(str(word)).vector)\n",
    "\n",
    "vectors = numpy.array(vectors)\n",
    "vectors = vectors.astype(numpy.float32)\n",
    "\n",
    "numpy.save(\"vectors.npy\", vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Clustering and Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy as np\n",
    "import faiss\n",
    "import pandas\n",
    "\n",
    "new_lexi = \"Clusters_KMEANS.csv\"\n",
    "\n",
    "########################################################\n",
    "\n",
    "# Load vectors\n",
    "vectors = np.load(\"vectors.npy\")\n",
    "\n",
    "# Normalize vectors\n",
    "print(\"Normalizing\\n\")\n",
    "vectors = normalize(vectors)\n",
    "\n",
    "# Reduce dimensions with PCA\n",
    "print(\"Reducing dimensions with PCA\\n\")\n",
    "pca = faiss.PCAMatrix(300, 50)\n",
    "pca.train(vectors)\n",
    "vectors = pca.apply_py(vectors)\n",
    "\n",
    "\n",
    "\n",
    "# Generate cluster labels\n",
    "print(\"Using KMEANS\\n\")\n",
    "algo = KMeans(init='k-means++', n_clusters=400, random_state=42)\n",
    "clusters = algo.fit_predict(vectors)\n",
    "\n",
    "# Save cluster labels\n",
    "numpy.save(\"clusters.npy\", clusters)\n",
    "\n",
    "# Append cluster labels to lexicon and save\n",
    "print(\"\\nWriting to file\")\n",
    "Lexicon = pandas.read_csv('LexiconFinal.csv', usecols=[1, 2])\n",
    "Lexicon['Clusters'] = clusters\n",
    "Lexicon.to_csv(new_lexi, header=['Word', 'ID', 'Cluster'], index=False)\n",
    "\n",
    "# Reduce dimensionality for 3D visualization\n",
    "print(\"Reducing dimensionality to 3D for visualization\\n\")\n",
    "tsne = TSNE(n_components=3, random_state=42, perplexity=30, max_iter=1000)\n",
    "vectors_3d = tsne.fit_transform(vectors)\n",
    "\n",
    "numpy.save(\"vectors_3D_tSNE.npy\", vectors_3d)\n",
    "\n",
    "\n",
    "print(\"Reducing dimensionality to 2D for visualization\\n\")\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30, max_iter=500)\n",
    "vectors_2d = tsne.fit_transform(vectors)\n",
    "numpy.save(\"vectors_2D_tSNE.npy\", vectors_2d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3D Point Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "Lexicon = pandas.read_csv(\"LexiconFinal.csv\", usecols=[1])\n",
    "words = Lexicon.iloc[:, 0]\n",
    "\n",
    "clusters = numpy.load(\"clusters.npy\")\n",
    "vectors_3d = numpy.load(\"vectors_tSNE.npy\")\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'x': vectors_3d[:, 0],\n",
    "    'y': vectors_3d[:, 1],\n",
    "    'z': vectors_3d[:, 2],\n",
    "    'Cluster': clusters,\n",
    "    'Word': words  # Word on hover\n",
    "})\n",
    "\n",
    "fig = px.scatter_3d(\n",
    "    df, \n",
    "    x='x', y='y', z='z', \n",
    "    color='Cluster',\n",
    "    color_continuous_scale=px.colors.qualitative.Set3,\n",
    "    title=\"3D Visualization of Clusters\",\n",
    "    opacity=0.7,\n",
    "    hover_name='Word'  # Use 'Word' column for hover text\n",
    ")\n",
    "fig.update_traces(marker=dict(size=3))\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "Lexicon = pandas.read_csv(\"LexiconFinal.csv\", usecols=[1])\n",
    "words = Lexicon.iloc[:, 0]\n",
    "\n",
    "clusters = numpy.load(\"clusters.npy\")\n",
    "vectors_3d = numpy.load(\"vectors_tSNE.npy\")\n",
    "\n",
    "df_2d = pd.DataFrame({\n",
    "    'x': vectors_3d[:, 0],  #first dimension\n",
    "    'y': vectors_3d[:, 1],  #second dimension\n",
    "    'Cluster': clusters,\n",
    "    'Word': words  #Word on hover\n",
    "})\n",
    "\n",
    "# Plot interactive 2D scatter\n",
    "fig_2d = px.scatter(\n",
    "    df_2d,\n",
    "    x='x', y='y',\n",
    "    color='Cluster',\n",
    "    color_continuous_scale=px.colors.qualitative.Set3,\n",
    "    title=\"2D Visualization of Clusters\",\n",
    "    hover_name='Word',  # Use 'Word' column for hover text\n",
    "    opacity=0.7\n",
    ")\n",
    "fig_2d.update_traces(marker=dict(size=5))\n",
    "\n",
    "fig_2d.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Barreling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import threading\n",
    "from collections import defaultdict\n",
    "import ijson\n",
    "import logging\n",
    "from typing import Dict, Any, Iterator\n",
    "import queue\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class BarrelIndexer:\n",
    "    def __init__(self, cluster_file: str, inverted_index_file: str, output_dir: str, chunk_size: int = 10000):\n",
    "        self.cluster_file = cluster_file\n",
    "        self.inverted_index_file = inverted_index_file\n",
    "        self.output_dir = output_dir\n",
    "        self.chunk_size = chunk_size\n",
    "        self.barrel_locks = defaultdict(threading.Lock)\n",
    "        self.barrel_buffers = defaultdict(dict)\n",
    "        self.buffer_size = 1000  # Number of entries to buffer before writing\n",
    "        \n",
    "    def load_clusters(self) -> Dict[int, int]:\n",
    "        \"\"\"Load cluster assignments efficiently\"\"\"\n",
    "        df = pd.read_csv(self.cluster_file, usecols=[1, 2])\n",
    "        return dict(zip(df['ID'], df['Cluster']))\n",
    "\n",
    "    def stream_inverted_index(self) -> Iterator[tuple]:\n",
    "        \"\"\"Stream the inverted index file in chunks\"\"\"\n",
    "        with open(self.inverted_index_file, 'rb') as file:\n",
    "            parser = ijson.parse(file)\n",
    "            current_key = None\n",
    "            current_value = None\n",
    "            \n",
    "            for prefix, event, value in parser:\n",
    "                if prefix.endswith('.key'):\n",
    "                    current_key = value\n",
    "                elif prefix.endswith('.value'):\n",
    "                    current_value = value\n",
    "                    if current_key is not None:\n",
    "                        yield (int(current_key), current_value)\n",
    "                        current_key = None\n",
    "                        current_value = None\n",
    "\n",
    "    def write_barrel_chunk(self, barrel: int, data: Dict[str, Any]):\n",
    "        \"\"\"Write a chunk of data to a barrel file with locking\"\"\"\n",
    "        barrel_file = os.path.join(self.output_dir, f\"{barrel}.json\")\n",
    "        \n",
    "        with self.barrel_locks[barrel]:\n",
    "            try:\n",
    "                existing_data = {}\n",
    "                if os.path.exists(barrel_file) and os.path.getsize(barrel_file) > 0:\n",
    "                    with open(barrel_file, 'r') as f:\n",
    "                        existing_data = json.load(f)\n",
    "                \n",
    "                existing_data.update(data)\n",
    "                \n",
    "                with open(barrel_file, 'w') as f:\n",
    "                    json.dump(existing_data, f, separators=(',', ':'))\n",
    "                    \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error writing to barrel {barrel}: {str(e)}\")\n",
    "                raise\n",
    "\n",
    "    def process_chunk(self, chunk: Dict[str, Any], barrels: Dict[int, int]):\n",
    "        \"\"\"Process a chunk of the inverted index\"\"\"\n",
    "        barrel_chunks = defaultdict(dict)\n",
    "        \n",
    "        for index, value in chunk.items():\n",
    "            index_int = int(index)\n",
    "            barrel = barrels.get(index_int)\n",
    "            \n",
    "            if barrel is None:\n",
    "                logger.warning(f\"No barrel found for index {index_int}\")\n",
    "                continue\n",
    "                \n",
    "            barrel_chunks[barrel][index] = value\n",
    "            \n",
    "        # Write accumulated chunks for each barrel\n",
    "        for barrel, data in barrel_chunks.items():\n",
    "            self.write_barrel_chunk(barrel, data)\n",
    "\n",
    "    def process(self, max_workers: int = 4):\n",
    "        \"\"\"Main processing method with parallel execution\"\"\"\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "        \n",
    "        # Load cluster assignments\n",
    "        logger.info(\"Loading cluster assignments...\")\n",
    "        barrels = self.load_clusters()\n",
    "        \n",
    "        # Create thread pool\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            current_chunk = {}\n",
    "            futures = []\n",
    "            \n",
    "            # Process the inverted index in streaming fashion\n",
    "            for index, value in tqdm(self.stream_inverted_index(), desc=\"Processing entries\"):\n",
    "                current_chunk[str(index)] = value\n",
    "                \n",
    "                if len(current_chunk) >= self.chunk_size:\n",
    "                    chunk_to_process = current_chunk\n",
    "                    current_chunk = {}\n",
    "                    futures.append(\n",
    "                        executor.submit(self.process_chunk, chunk_to_process, barrels)\n",
    "                    )\n",
    "                    \n",
    "            # Process any remaining entries\n",
    "            if current_chunk:\n",
    "                futures.append(\n",
    "                    executor.submit(self.process_chunk, current_chunk, barrels)\n",
    "                )\n",
    "            \n",
    "            # Wait for all tasks to complete\n",
    "            for future in tqdm(futures, desc=\"Waiting for tasks\"):\n",
    "                future.result()\n",
    "\n",
    "def main():\n",
    "    indexer = BarrelIndexer(\n",
    "        cluster_file=\".\\\\Barrels\\\\Clusters_KMEANS.csv\",\n",
    "        inverted_index_file=\"Inverted_index.json\",\n",
    "        output_dir=\".\\\\Barrels\\\\Index_Barrels\",\n",
    "        chunk_size=10000\n",
    "    )\n",
    "    indexer.process(max_workers=4)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonEnvironment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
